{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["pFOnoAnNUer0"],"toc_visible":true,"authorship_tag":"ABX9TyMDZOTk4dywmOb1lrL3JE4M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Connection, install, import"],"metadata":{"id":"pFOnoAnNUer0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPBwWuGDR7bh"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd drive/MyDrive/'...Your path...'/"],"metadata":{"id":"5Gs2e1YKUp-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"id":"Lu_ZXt_sUqBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# used packages\n","import pandas as pd\n","import transformers\n","from tqdm import tqdm\n","import torch\n","from torch.nn import MSELoss, CrossEntropyLoss\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n","    TensorDataset)\n","from nltk.tokenize import sent_tokenize\n","import logging\n","import nltk\n","nltk.download('punkt')\n","import numpy as np\n","logger = logging.getLogger(__name__)\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"],"metadata":{"id":"ePsim1rHUqEI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# adobe apple ibm microsoft nvidia qualcomm salesforce servicenow\n","\n","company_name = 'adobe'\n","path = f'processed_tweets/{company_name}.csv'\n","df = pd.read_csv(path, lineterminator='\\n') # read downloaded tweets\n","df.head()"],"metadata":{"id":"b3Xi4Rg1ashW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BERT"],"metadata":{"id":"rxlPrEbJapw2"}},{"cell_type":"code","source":["classifier = transformers.pipeline('sentiment-analysis')"],"metadata":{"id":"h8MSgnISa57W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['BERT_sent'] = ''\n","n = 5000 # tweets at once\n","num = round(df.shape[0] / n)\n","st = 0\n","\n","# BERT classification\n","for i in range(num):\n","  result = []\n","  result = pd.DataFrame(classifier(list(df.loc[st:st+n-1, 'text_format'])))\n","  df.loc[st:st+n, 'BERT_sent']  = result.loc[:,'label']\n","  st += n"],"metadata":{"id":"VbVNbd92a5-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cleanup_nums = {\"BERT_sent\": {\"NEGATIVE\": 0, \"POSITIVE\": 1}} # replace NEGATIVE on 0, POSITIVE on 1\n","df = df.replace(cleanup_nums)"],"metadata":{"id":"kQzFb_FRa6A-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8sroc3pEr0WE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FinBERT"],"metadata":{"id":"u20pdFYVbo-z"}},{"cell_type":"code","source":["#source https://github.com/ProsusAI/finBERT/tree/master/finbert\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text, label=None, agree=None):\n","        \"\"\"\n","        Constructs an InputExample\n","        Parameters\n","        ----------\n","        guid: str\n","            Unique id for the examples\n","        text: str\n","            Text for the first sequence.\n","        label: str, optional\n","            Label for the example.\n","        agree: str, optional\n","            For FinBERT , inter-annotator agreement level.\n","        \"\"\"\n","        self.guid = guid\n","        self.text = text\n","        self.label = label\n","        self.agree = agree\n","\n","def softmax(x):\n","    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","    e_x = np.exp(x - np.max(x, axis=1)[:, None])\n","    return e_x / np.sum(e_x, axis=1)[:, None]\n","\n","class InputFeatures(object):\n","    \"\"\"\n","    A single set of features for the data.\n","    \"\"\"\n","\n","    def __init__(self, input_ids, attention_mask, token_type_ids, label_id, agree=None):\n","        self.input_ids = input_ids\n","        self.attention_mask = attention_mask\n","        self.token_type_ids = token_type_ids\n","        self.label_id = label_id\n","        self.agree = agree\n","\n","def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, mode='classification'):\n","    \"\"\"\n","    Loads a data file into a list of InputBatch's. With this function, the InputExample's are converted to features\n","    that can be used for the model. Text is tokenized, converted to ids and zero-padded. Labels are mapped to integers.\n","    Parameters\n","    ----------\n","    examples: list\n","        A list of InputExample's.\n","    label_list: list\n","        The list of labels.\n","    max_seq_length: int\n","        The maximum sequence length.\n","    tokenizer: BertTokenizer\n","        The tokenizer to be used.\n","    mode: str, optional\n","        The task type: 'classification' or 'regression'. Default is 'classification'\n","    Returns\n","    -------\n","    features: list\n","        A list of InputFeature's, which is an InputBatch.\n","    \"\"\"\n","\n","    if mode == 'classification':\n","        label_map = {label: i for i, label in enumerate(label_list)}\n","        label_map[None] = 9090\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        tokens = tokenizer.tokenize(example.text)\n","\n","        if len(tokens) > max_seq_length - 2:\n","            tokens = tokens[:(max_seq_length // 4) - 1] + tokens[\n","                                                          len(tokens) - (3 * max_seq_length // 4) + 1:]\n","\n","        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n","\n","        token_type_ids = [0] * len(tokens)\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        attention_mask = [1] * len(input_ids)\n","\n","        padding = [0] * (max_seq_length - len(input_ids))\n","        input_ids += padding\n","        attention_mask += padding\n","\n","\n","        token_type_ids += padding\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(attention_mask) == max_seq_length\n","        assert len(token_type_ids) == max_seq_length\n","\n","        if mode == 'classification':\n","            label_id = label_map[example.label]\n","        elif mode == 'regression':\n","            label_id = float(example.label)\n","        else:\n","            raise ValueError(\"The mode should either be classification or regression. You entered: \" + mode)\n","\n","        agree = example.agree\n","        mapagree = {'0.5': 1, '0.66': 2, '0.75': 3, '1.0': 4}\n","        try:\n","            agree = mapagree[agree]\n","        except:\n","            agree = 0\n","\n","        if ex_index < 1:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\" % (example.guid))\n","            logger.info(\"tokens: %s\" % \" \".join(\n","                [str(x) for x in tokens]))\n","            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n","            logger.info(\n","                \"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n","            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n","\n","        features.append(\n","            InputFeatures(input_ids=input_ids,\n","                          attention_mask=attention_mask,\n","                          token_type_ids=token_type_ids,\n","                          label_id=label_id,\n","                          agree=agree))\n","    return features\n","\n","def chunks(l, n):\n","    \"\"\"\n","    Simple utility function to split a list into fixed-length chunks.\n","    Parameters\n","    ----------\n","    l: list\n","        given list\n","    n: int\n","        length of the sequence\n","    \"\"\"\n","    for i in range(0, len(l), n):\n","        # Create an index range for l of n items:\n","        yield l[i:i + n]\n","\n","def predict(text, model, write_to_csv=False, path=None, use_gpu=False, gpu_name='cuda:0', batch_size=10):\n","    \"\"\"\n","    Predict sentiments of sentences in a given text. The function first tokenizes sentences, make predictions and write\n","    results.\n","    Parameters\n","    ----------\n","    text: string\n","        text to be analyzed\n","    model: BertForSequenceClassification\n","        path to the classifier model\n","    write_to_csv (optional): bool\n","    path (optional): string\n","        path to write the string\n","    use_gpu: (optional): bool\n","        enables inference on GPU\n","    gpu_name: (optional): string\n","        multi-gpu support: allows specifying which gpu to use\n","    batch_size: (optional): int\n","        size of batching chunks\n","    \"\"\"\n","    model.eval()\n","\n","    sentences = text\n","\n","    device = gpu_name if use_gpu and torch.cuda.is_available() else \"cpu\"\n","    logging.info(\"Using device: %s \" % device)\n","    label_list = ['positive', 'negative', 'neutral']\n","    label_dict = {0: 'positive', 1: 'negative', 2: 'neutral'}\n","    result = pd.DataFrame(columns=['prediction', 'sentiment_score'])\n","    #  result = pd.DataFrame(columns=['sentence', 'logit', 'prediction', 'sentiment_score'])\n","\n","    for batch in tqdm(chunks(sentences, batch_size), ascii=True, desc='finBERT processing'):\n","    # for batch in chunks(sentences, batch_size):\n","        examples = [InputExample(str(i), sentence) for i, sentence in enumerate(batch)]\n","\n","        features = convert_examples_to_features(examples, label_list, 64, tokenizer)\n","\n","        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long).to(device)\n","        all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long).to(device)\n","        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long).to(device)\n","\n","        with torch.no_grad():\n","            model     = model.to(device)\n","\n","            logits = model(all_input_ids, all_attention_mask, all_token_type_ids)[0]\n","            logging.info(logits)\n","            logits = softmax(np.array(logits.cpu()))\n","            sentiment_score = pd.Series(logits[:, 0] - logits[:, 1])\n","            predictions = np.squeeze(np.argmax(logits, axis=1))\n","\n","            batch_result = {\n","                            # 'sentence': batch,\n","                            # 'logit': list(logits),\n","                            'prediction': predictions,\n","                            'sentiment_score': sentiment_score}\n","\n","            batch_result = pd.DataFrame(batch_result)\n","            result = pd.concat([result, batch_result], ignore_index=True)\n","\n","    result['prediction'] = result.prediction.apply(lambda x: label_dict[x])\n","    if write_to_csv:\n","        result.to_csv(path, sep=',', index=False)\n","\n","    return result"],"metadata":{"id":"YVpT0ZGnbrlt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['fin_sent'] = ''\n","df['fin_sent_score'] = 0\n","\n","batchSize = 4000\n","n = 5000 # tweets at once\n","num = round(df.shape[0] / n)\n","st = 0\n","\n","# FinBERT classification\n","for i in range(num):\n","  result = []\n","  result = predict(df['text_format'][st:st+n], model, batch_size = batchSize)\n","  df.loc[st:st+n-1, 'fin_sent'] = list(result.loc[:,'prediction'])\n","  df.loc[st:st+n-1, 'fin_sent_score'] = list(result.loc[:,'sentiment_score'])\n","  st += n"],"metadata":{"id":"7rIcGW5Abrog"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# score >=0 is classified as 1, else 0\n","df['finsent'] = 0\n","df.loc[df[\"fin_sent_score\"] < 0, \"finsent\"] = 0\n","df.loc[df[\"fin_sent_score\"] >= 0, \"finsent\"] = 1\n","df.head()"],"metadata":{"id":"aISusUiocNB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cleanup_nums = {\"fin_sent\": {\"neutral\": 0, \"positive\": 1, \"negative\": -1}} # replace negative on -1, positive on 1, neutral on 0\n","df = df.replace(cleanup_nums)\n","df.head()"],"metadata":{"id":"ZHs5VipkbrrF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df[['date', 'text_format',\t'BERT_sent',\t'finsent']]"],"metadata":{"id":"_WTteImnTBmp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = f'classified_tweets/{company_name}.csv'\n","df.to_csv(path, header=True, index=False)"],"metadata":{"id":"IBV9qLawcRZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"etdYf0hHoXNE"},"execution_count":null,"outputs":[]}]}