{"cells":[{"cell_type":"markdown","source":["# Connection, install, import"],"metadata":{"id":"k0KocENV5k3x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dlAAYIwkIRs"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd drive/MyDrive/'...Your path...'/"],"metadata":{"id":"wmVQ-U5BS9Fu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJGpjQo2lG-B"},"outputs":[],"source":["pip install wordninja"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRHz4HaXk5bp"},"outputs":[],"source":["# used packages\n","import csv\n","import pandas as pd\n","import re\n","import pickle\n","import wordninja\n","from tqdm import tqdm\n","from tokenize import tokenize\n","\n","from nltk.tokenize import word_tokenize\n","import nltk.classify.util\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"]},{"cell_type":"markdown","source":["# preprocessing functions"],"metadata":{"id":"yHF9Q2FZ5voi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYTWQFg7kRLr"},"outputs":[],"source":["# words will be delete\n","stop_words = [\n"," 'but',\n"," 'only',\n"," 'retweet']\n","\n","# contractions decoding\n","CONTRACTION_MAP = {}\n","with open('dictionary/Contraction_Dict.p', 'rb') as fp:\n","    CONTRACTION_MAP = pickle.load(fp)\n","\n","def convert_contraction_to_word(text):\n","    for cont in CONTRACTION_MAP:\n","        text = re.sub(r' '+cont+'\\W', f' {\" \".join(CONTRACTION_MAP[cont].split())} ', text)\n","    return text\n","\n","#emoji decoding\n","emojis = {}\n","with open('dictionary/Emoji.pickle', 'rb') as fp:\n","    emojis = pickle.load(fp)\n","\n","def convert_smiley_to_word(text):\n","    for emot in emojis:\n","        text = re.sub(r'('+emot+')', f' {\" \".join(emojis[emot].split())}', text)\n","    return text\n","\n","#emoticons decoding\n","emoticons = {}\n","with open('dictionary/emoticons.pickle', 'rb') as fp:\n","    emoticons = pickle.load(fp)\n","\n","def convert_emoticons_to_word(text):\n","    for emot in emoticons:\n","        text = re.sub(r'('+emot+')', f' {\" \".join(emoticons[emot].split())}', text)\n","    return text"]},{"cell_type":"markdown","source":["# preprocessing tweets"],"metadata":{"id":"Hx88u4gE-aRB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5ahfSeOlQAp"},"outputs":[],"source":["# adobe apple ibm microsoft nvidia qualcomm salesforce servicenow\n","\n","company_name = 'adobe'\n","path = f'tweets/{company_name}.csv'\n","df = pd.read_csv(path, lineterminator='\\n') # read downloaded tweets\n","df = df.sort_values(by='date')\n","df = df.drop_duplicates(subset=['text', 'username']) #???\n","df = df.reset_index(drop=True)\n","df['text_format'] =''\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUfRxMdjkWIJ"},"outputs":[],"source":["# checking null text values\n","# df = df[df['text'].isnull().values == False]\n","df[df['text'].isnull().values == True]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5fcyJiakYnh"},"outputs":[],"source":["# checking null date values\n","# df = df[df['date'].isnull().values == False]\n","df[df['date'].isnull().values == True]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9E372FRWkDJ6"},"outputs":[],"source":["num = df.shape[0]\n","\n","for i in tqdm(range(0, num), ascii=True, desc='pre-processing text'):\n","  text = df.loc[i, 'text']\n","  text = text.lower() # lowcase\n","  text = re.sub(r\"&amp|&gt|(?:https?\\://)\\S+|\\@\\S+\", \"\", text) # delete: &amp, &gt, https://, @\n","  text = re.sub(r\"\\.{2,}\", \". \", text) # delete ellipsis\n","  text = convert_contraction_to_word(text) # contractions decoding\n","  text = ' '.join([CONTRACTION_MAP.get(item, item) for item in text.split()])\n","  text = convert_emoticons_to_word(text)  #emoticons decoding\n","  text = convert_smiley_to_word(text) #emoji decoding\n","  text = \" \".join([re.sub(r\"\\#\\S+\", \" \".join(wordninja.split(w)), w)  for w in text.split()]) #hashtags decoding\n","  text = \" \".join([w for w in text.split() if not w in stop_words]) # delete stop words\n","  text = re.sub(\"[^a-zA-Z0-9();:\\\".%?!/,'\\- ]\", \"\", text) # delete symbols except listed ones\n","\n","  df.loc[i, 'text_format'] = text\n","\n","  # if len(text.split())<3:\n","  #   less_three.append(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SaoD88hElDSi"},"outputs":[],"source":["less_three = []\n","num = df.shape[0]\n","for i in tqdm(range(0, num), ascii=True, desc='less than 3words text'):\n","  if len(str(df.loc[i, 'text_format']).split())<3:\n","    less_three.append(i)\n","\n","df = df.drop(labels = less_three ,axis = 0) # delete observations that less than 3 words in\n","df = df.reset_index(drop=True)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slCWH_RWrGe8"},"outputs":[],"source":["# saved\n","path = f'processed_tweets/{company_name}.csv'\n","df.to_csv(path, header=True, index=False)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvHmMvbIHNkYwXuacHYja4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}